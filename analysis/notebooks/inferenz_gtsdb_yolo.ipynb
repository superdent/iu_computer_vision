{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e1cc031",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import torch\n",
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "import math\n",
    "from IPython.display import display, clear_output\n",
    "from PIL import Image\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f336f92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fertig: altendorfer_2.mp4\n"
     ]
    }
   ],
   "source": [
    "# Video verkleinern\n",
    "import cv2\n",
    "\n",
    "input_path = \"altendorfer_1.mp4\"\n",
    "output_path = \"altendorfer_2.mp4\"\n",
    "scale = 0.3  # 0.3 = 30% der Originalgröße\n",
    "\n",
    "cap = cv2.VideoCapture(input_path)\n",
    "w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH) * scale)\n",
    "h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT) * scale)\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "out = cv2.VideoWriter(output_path, fourcc, fps, (w, h))\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    frame = cv2.resize(frame, (w, h), interpolation=cv2.INTER_AREA)\n",
    "    out.write(frame)\n",
    "\n",
    "cap.release()\n",
    "out.release()\n",
    "print(\"Fertig:\", output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a55bc112",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pfade/Dateinamen (plattformunabhängig)\n",
    "MODEL_PATH   = Path(\"best.pt\")            # YOLO11-Gewichte (.pt)\n",
    "VIDEO_PATH   = Path(\"altendorfer_2.mp4\")     # Eingabevideo\n",
    "OUTPUT_VIDEO = Path(f\"{VIDEO_PATH.stem}_bboxes.mp4\")  # Ausgabevideo\n",
    "\n",
    "# Inferenz-Parameter:\n",
    "CONF_THRES     = 0.25   # Mindest-Konfidenz für Detections\n",
    "IOU_THRES      = 0.45   # IoU-Schwelle für NMS (Dubletten-Unterdrückung)\n",
    "DISPLAY_EVERY  = 10     # Jeden n-ten Frame inline anzeigen (0 = nie)\n",
    "\n",
    "# Device-Handhabung:\n",
    "DEVICE_AUTO = True      # True = automatisch wählen (cuda falls verfügbar, sonst cpu)\n",
    "DEVICE      = \"cpu\"     # Wird nur benutzt, wenn DEVICE_AUTO = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8fb7df45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch: 2.8.0+cpu | OpenCV: 4.12.0 | Device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Device bestimmen (fail-safe)\n",
    "_device = (\"cuda\" if torch.cuda.is_available() else \"cpu\") if DEVICE_AUTO else DEVICE\n",
    "\n",
    "# Sanity-Checks\n",
    "if not MODEL_PATH.exists():\n",
    "    raise FileNotFoundError(f\"Modell nicht gefunden: {MODEL_PATH}\")\n",
    "if not VIDEO_PATH.exists():\n",
    "    raise FileNotFoundError(f\"Video nicht gefunden: {VIDEO_PATH}\")\n",
    "\n",
    "# YOLOv11 laden\n",
    "model = YOLO(str(MODEL_PATH))  # Gerät wird später beim Predict übergeben\n",
    "print(f\"Torch: {torch.__version__} | OpenCV: {cv2.__version__} | Device: {_device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a8460a7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video: C:\\Develop\\Python\\PyCharmProjects\\iu_computer_vision\\analysis\\notebooks\\altendorfer_2.mp4 | 576x324@29.97 -> Ausgabe: C:\\Develop\\Python\\PyCharmProjects\\iu_computer_vision\\analysis\\notebooks\\altendorfer_2_bboxes.mp4\n"
     ]
    }
   ],
   "source": [
    "_video_abs = Path(VIDEO_PATH).resolve()\n",
    "_out_abs   = Path(OUTPUT_VIDEO).resolve()\n",
    "\n",
    "if not _video_abs.is_file():\n",
    "    raise FileNotFoundError(f\"Video-Datei nicht gefunden: {_video_abs}\")\n",
    "\n",
    "cap = cv2.VideoCapture(str(_video_abs))\n",
    "if not cap.isOpened():\n",
    "    raise RuntimeError(f\"Konnte Video nicht öffnen: {_video_abs}\")\n",
    "\n",
    "fps = cap.get(cv2.CAP_PROP_FPS) or 25.0\n",
    "w   = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "h   = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "if w == 0 or h == 0:\n",
    "    raise RuntimeError(f\"Ungültige Videodimensionen für: {_video_abs}\")\n",
    "\n",
    "fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "writer = cv2.VideoWriter(str(_out_abs), fourcc, fps, (w, h))\n",
    "\n",
    "def _show(frame_bgr):\n",
    "    rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)\n",
    "    clear_output(wait=True); display(Image.fromarray(rgb))\n",
    "\n",
    "print(f\"Video: {_video_abs} | {w}x{h}@{fps:.2f} -> Ausgabe: {_out_abs}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05032535",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] load_state_dict: missing: ['features.1.weight', 'features.1.bias', 'features.1.running_mean', 'features.1.running_var', 'features.4.weight', 'features.4.bias', 'features.5.weight', 'features.5.bias', 'features.5.running_mean', 'features.5.running_var', 'features.8.weight', 'features.8.bias', 'features.9.weight', 'features.9.bias', 'features.9.running_mean', 'features.9.running_var', 'classifier.4.weight', 'classifier.4.bias'] unexpected: ['features.3.weight', 'features.3.bias', 'features.6.weight', 'features.6.bias', 'classifier.3.weight', 'classifier.3.bias']\n",
      "MicronNetPlus geladen & classify_sign bereit.\n"
     ]
    }
   ],
   "source": [
    "# MicronNetPlus laden & für ROIs nutzbar machen – NUR torch, cv2, numpy\n",
    "import torch, torch.nn as nn\n",
    "import cv2, numpy as np\n",
    "\n",
    "# ==== Architektur aus dem Training (ohne externe Module) ====\n",
    "IMG_SIZE  = 48\n",
    "N_CLASSES = 43\n",
    "USE_MISH  = True  # falls du im Training False hattest, hier auch auf False setzen\n",
    "\n",
    "class Mish(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x * torch.tanh(torch.nn.functional.softplus(x))\n",
    "\n",
    "def act_layer():\n",
    "    return Mish() if USE_MISH else nn.ReLU(inplace=True)\n",
    "\n",
    "class MicronNetPlus(nn.Module):\n",
    "    def __init__(self, n_classes=43):\n",
    "        super().__init__()\n",
    "        A = act_layer\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, 3, padding=1), nn.BatchNorm2d(32), A(), nn.MaxPool2d(2),\n",
    "            nn.Conv2d(32, 64, 3, padding=1), nn.BatchNorm2d(64), A(), nn.MaxPool2d(2),\n",
    "            nn.Conv2d(64, 128, 3, padding=1), nn.BatchNorm2d(128), A(), nn.MaxPool2d(2),\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128*6*6, 256), A(), nn.Dropout(0.4),\n",
    "            nn.Linear(256, n_classes)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.classifier(self.features(x))\n",
    "\n",
    "# ==== Checkpoint laden (speichert unter \"model\") ====\n",
    "path_ckpt = r\"micronnet_best.pt\"\n",
    "ckpt = torch.load(path_ckpt, map_location=\"cpu\")\n",
    "key = \"model\" if \"model\" in ckpt else (\n",
    "      \"model_state_dict\" if \"model_state_dict\" in ckpt else (\n",
    "      \"state_dict\" if \"state_dict\" in ckpt else None))\n",
    "if key is None:\n",
    "    raise ValueError(f\"Checkpoint-Schlüssel nicht gefunden. Verfügbare Keys: {list(ckpt.keys())}\")\n",
    "\n",
    "model_cls = MicronNetPlus(N_CLASSES)\n",
    "# Eventuelles 'module.'-Präfix entfernen\n",
    "sd = ckpt[key]\n",
    "if any(k.startswith(\"module.\") for k in sd.keys()):\n",
    "    sd = {k.replace(\"module.\",\"\",1): v for k,v in sd.items()}\n",
    "missing, unexpected = model_cls.load_state_dict(sd, strict=False)\n",
    "if missing or unexpected:\n",
    "    print(\"[WARN] load_state_dict:\", \"missing:\", missing, \"unexpected:\", unexpected)\n",
    "model_cls.eval()\n",
    "\n",
    "# ==== Klassenliste (optional echte GTSRB-Namen einsetzen) ====\n",
    "GTSRB_CLASSES = [str(i) for i in range(N_CLASSES)]\n",
    "\n",
    "@torch.no_grad()\n",
    "def classify_sign(crop_bgr: np.ndarray) -> str:\n",
    "    # Preprocessing wie im Training: Resize 48x48, ToTensor, Normalize(mean=std=0.5)\n",
    "    img = cv2.resize(crop_bgr, (IMG_SIZE, IMG_SIZE), interpolation=cv2.INTER_AREA)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB).astype(np.float32) / 255.0\n",
    "    img = (img - 0.5) / 0.5\n",
    "    t = torch.from_numpy(img).permute(2, 0, 1).unsqueeze(0)  # 1x3x48x48\n",
    "    logits = model_cls(t)\n",
    "    cls_id = int(torch.argmax(logits, dim=1))\n",
    "    return GTSRB_CLASSES[cls_id]\n",
    "\n",
    "# Kurztest\n",
    "_ = classify_sign(np.zeros((60,60,3), dtype=np.uint8))\n",
    "print(\"MicronNetPlus geladen & classify_sign bereit.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad6effa",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'eval'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# --- MicronNet laden ---\u001b[39;00m\n\u001b[32m      6\u001b[39m micronnet = torch.load(\u001b[33m\"\u001b[39m\u001b[33mmicronnet_best.pt\u001b[39m\u001b[33m\"\u001b[39m, map_location=\u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[43mmicronnet\u001b[49m\u001b[43m.\u001b[49m\u001b[43meval\u001b[49m()\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# Optional: echte Klassennamen einsetzen\u001b[39;00m\n\u001b[32m     10\u001b[39m GTSRB_CLASSES = [\u001b[38;5;28mstr\u001b[39m(i) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m43\u001b[39m)]\n",
      "\u001b[31mAttributeError\u001b[39m: 'dict' object has no attribute 'eval'"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mDer Kernel ist beim Ausführen von Code in der aktuellen Zelle oder einer vorherigen Zelle abgestürzt. \n",
      "\u001b[1;31mBitte überprüfen Sie den Code in der/den Zelle(n), um eine mögliche Fehlerursache zu identifizieren. \n",
      "\u001b[1;31mKlicken Sie <a href='https://aka.ms/vscodeJupyterKernelCrash'>hier</a>, um weitere Informationen zu erhalten. \n",
      "\u001b[1;31mWeitere Informationen finden Sie unter Jupyter <a href='command:jupyter.viewOutput'>Protokoll</a>."
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 32   # größer = mehr Durchsatz, mehr Latenz\n",
    "frame_idx = 0\n",
    "frames = []\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    frames.append(frame)\n",
    "\n",
    "    if len(frames) == BATCH_SIZE:\n",
    "        results = model.predict(\n",
    "            source=frames,\n",
    "            conf=CONF_THRES,\n",
    "            iou=IOU_THRES,\n",
    "            device=_device,\n",
    "            verbose=False\n",
    "        )\n",
    "\n",
    "        for res, frm in zip(results, frames):\n",
    "            boxes = res.boxes\n",
    "            if boxes and len(boxes) > 0:  # nur zeigen, wenn Detektion vorhanden\n",
    "                for b in boxes:\n",
    "                    x1, y1, x2, y2 = map(int, b.xyxy[0].tolist())\n",
    "                    conf = float(b.conf[0])\n",
    "                    cv2.rectangle(frm, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "                    cv2.putText(frm, f\"{conf:.2f}\", (x1, max(0, y1-5)),\n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0,255,0), 1, cv2.LINE_AA)\n",
    "                _show(frm)  # nur hier, kein DISPLAY_EVERY\n",
    "            writer.write(frm)\n",
    "            frame_idx += 1\n",
    "        frames.clear()\n",
    "\n",
    "# Restbatch\n",
    "if frames:\n",
    "    results = model.predict(source=frames, conf=CONF_THRES, iou=IOU_THRES, device=_device, verbose=False)\n",
    "    for res, frm in zip(results, frames):\n",
    "        boxes = res.boxes\n",
    "        if boxes and len(boxes) > 0:\n",
    "            for b in boxes:\n",
    "                x1, y1, x2, y2 = map(int, b.xyxy[0].tolist())\n",
    "                conf = float(b.conf[0])\n",
    "                cv2.rectangle(frm, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "                cv2.putText(frm, f\"{conf:.2f}\", (x1, max(0, y1-5)),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0,255,0), 1, cv2.LINE_AA)\n",
    "            _show(frm)\n",
    "        writer.write(frm)\n",
    "        frame_idx += 1\n",
    "\n",
    "print(f\"Fertig: {frame_idx} Frames verarbeitet.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cc85d520",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ausgabe gespeichert unter: generated_video_1_bboxes.mp4\n"
     ]
    }
   ],
   "source": [
    "# Cleanup\n",
    "cap.release()\n",
    "writer.release()\n",
    "cv2.destroyAllWindows()\n",
    "print(f\"Ausgabe gespeichert unter: {OUTPUT_VIDEO}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_notebooks",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
